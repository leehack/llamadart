name: llamadart
description: A Dart/Flutter plugin for llama.cpp - run LLM inference on any platform using GGUF models
version: 0.3.0+b7883
homepage: https://github.com/leehack/llamadart
repository: https://github.com/leehack/llamadart
issue_tracker: https://github.com/leehack/llamadart/issues
topics:
  - llama
  - llm
  - ai
  - inference
  - gguf

environment:
  sdk: ^3.10.7
  flutter: ^3.38.0

dependencies:
  ffi: ^2.1.0
  path: ^1.8.3
  meta: ^1.11.0
  http: ^1.1.0
  web: ^1.0.0
  flutter:
    sdk: flutter
  code_assets: ^0.19.7
  hooks: ^0.20.1
  native_toolchain_c: ^0.17.2
  logging: ^1.3.0
  native_assets_cli: ^0.18.0

dev_dependencies:
  archive: ^4.0.7
  ffigen: ^20.1.1
  lints: ^6.0.0

  test: ^1.29.0

# No plugin section needed for Pure Native Asset FFI package


